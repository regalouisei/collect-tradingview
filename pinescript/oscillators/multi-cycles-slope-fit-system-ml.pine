// This Pine Script™ code is subject to the terms of the Mozilla Public License 2.0
// =============================================================================
// MULTI CYCLES PREDICTIVE SYSTEM ML - SLOPE FITTING VERSION
// =============================================================================
// - GBM aggressively curve-fits cycle slope to price slope
// - 20-day rolling window for training
// - Weights algorithms by how well their cycle matches price direction
// - 13 Cycle Algorithms with adaptive learning
// =============================================================================
//@jaydesaigu
//@version=5
indicator("Multi Cycles Slope-Fit System ML", 
     shorttitle="MCPS-Slope", 
     overlay=false,
     max_bars_back=500,
     max_labels_count=500)

// =============================================================================
// CONFIGURATION
// =============================================================================

group_general = "═══ GENERAL SETTINGS ═══"
fit_window = input.int(20, "Fitting Window (Days)", minval=10, maxval=50, group=group_general)
cycle_period_min = input.int(5, "Min Cycle Period", minval=3, maxval=10, group=group_general)
cycle_period_max = input.int(15, "Max Cycle Period", minval=10, maxval=30, group=group_general)

group_gbm = "═══ GBM SETTINGS ═══"
gbm_learning_rate = input.float(0.25, "GBM Learning Rate", minval=0.05, maxval=0.5, step=0.01, group=group_gbm)
gbm_memory_decay = input.float(0.92, "Memory Decay (aggressive)", minval=0.85, maxval=0.99, step=0.01, group=group_gbm)
gbm_regularization = input.float(0.05, "Regularization Strength", minval=0.01, maxval=0.2, step=0.01, group=group_gbm)
gbm_temperature = input.float(0.5, "Softmax Temperature (aggressive)", minval=0.2, maxval=1.5, step=0.1, group=group_gbm)
gbm_smoothing = input.float(0.3, "Weight Smoothing (low=faster)", minval=0.0, maxval=0.8, step=0.05, group=group_gbm)

group_weights = "═══ WEIGHT CONSTRAINTS ═══"
max_single_weight = input.float(0.55, "Max Single Algorithm Weight", minval=0.3, maxval=0.7, step=0.05, group=group_weights)
max_top3_weight = input.float(0.92, "Max Top 3 Combined Weight", minval=0.7, maxval=0.98, step=0.02, group=group_weights)
min_weight_floor = input.float(0.005, "Minimum Weight Floor", minval=0.001, maxval=0.03, step=0.002, group=group_weights)

group_slope = "═══ SLOPE FITTING ═══"
slope_lookback = input.int(5, "Slope Calculation Lookback", minval=2, maxval=15, group=group_slope)
correlation_weight = input.float(0.6, "Correlation vs Direction Weight", minval=0.0, maxval=1.0, step=0.1, group=group_slope)
direction_bonus = input.float(2.0, "Direction Match Bonus", minval=1.0, maxval=5.0, step=0.5, group=group_slope)

group_visual = "═══ VISUALIZATION ═══"
show_cycle_line = input.bool(true, "Show Combined Cycle Line", group=group_visual)
show_price_overlay = input.bool(true, "Show Normalized Price", group=group_visual)
show_individual_cycles = input.bool(false, "Show Individual Algorithm Cycles", group=group_visual)
show_weight_table = input.bool(true, "Show Weight Table", group=group_visual)
show_gbm_stats = input.bool(true, "Show GBM Statistics", group=group_visual)
warmup_period = input.int(30, "Warmup Period (bars)", minval=20, maxval=100, group=group_visual)

// =============================================================================
// CONSTANTS
// =============================================================================

NUM_MODELS = 13
DEFAULT_WEIGHT = 1.0 / NUM_MODELS

// =============================================================================
// UTILITY FUNCTIONS
// =============================================================================

f_nz(float val, float replacement) =>
    na(val) ? replacement : val

f_clip(float val, float minVal, float maxVal) =>
    math.max(math.min(f_nz(val, (minVal + maxVal) / 2), maxVal), minVal)

f_safe_div(float num, float denom, float default_val) =>
    denom == 0 or na(denom) or na(num) ? default_val : num / denom

// Calculate slope over N periods (normalized)
f_slope(series float src, int length) =>
    float sum_xy = 0.0
    float sum_x = 0.0
    float sum_y = 0.0
    float sum_x2 = 0.0
    int n = length
    for i = 0 to length - 1
        float x = float(length - 1 - i)
        float y = f_nz(src[i], src)
        sum_xy += x * y
        sum_x += x
        sum_y += y
        sum_x2 += x * x
    float denom = n * sum_x2 - sum_x * sum_x
    denom != 0 ? (n * sum_xy - sum_x * sum_y) / denom : 0.0

// Rolling correlation between two series
f_rolling_corr(series float x, series float y, int length) =>
    float corr = ta.correlation(x, y, length)
    f_nz(corr, 0.0)

// Softmax with temperature
f_softmaxTemp(array<float> weights, float temperature) =>
    int size = array.size(weights)
    array<float> result = array.new_float(size, DEFAULT_WEIGHT)
    
    float maxVal = -999999.0
    for i = 0 to size - 1
        float w = array.get(weights, i)
        if not na(w) and w > maxVal
            maxVal := w
    
    if maxVal == -999999.0
        maxVal := 0.0
    
    float sumExp = 0.0
    float safeTemp = math.max(f_nz(temperature, 1.0), 0.1)
    
    for i = 0 to size - 1
        float w = f_nz(array.get(weights, i), 0.0)
        float diff = (w - maxVal) / safeTemp
        float clippedDiff = math.max(math.min(diff, 50.0), -50.0)
        float expVal = math.exp(clippedDiff)
        array.set(result, i, expVal)
        sumExp += expVal
    
    float safeSumExp = math.max(sumExp, 0.0001)
    for i = 0 to size - 1
        float normalized = array.get(result, i) / safeSumExp
        array.set(result, i, f_nz(normalized, DEFAULT_WEIGHT))
    
    result

// Calculate entropy
f_calcEntropy(array<float> weights) =>
    int size = array.size(weights)
    float entropy = 0.0
    for i = 0 to size - 1
        float w = f_nz(array.get(weights, i), DEFAULT_WEIGHT)
        if w > 0.0001
            entropy -= w * math.log(w)
    f_nz(entropy / math.log(float(size)), 0.5)

// Normalize to [-1, 1] range
f_normalize_cycle(float val, float minVal, float maxVal) =>
    float val_range = maxVal - minVal
    val_range > 0 ? 2 * (val - minVal) / val_range - 1 : 0.0

// Normalize probability to [0, 1]
f_normalize_prob(float val, float minVal, float maxVal) =>
    float val_range = maxVal - minVal
    val_range > 0 and not na(val) and not na(minVal) and not na(maxVal) ? 
         f_clip((val - minVal) / val_range, 0.0, 1.0) : 0.5

// =============================================================================
// PRICE NORMALIZATION FOR COMPARISON
// =============================================================================

price_min = ta.lowest(close, fit_window)
price_max = ta.highest(close, fit_window)
price_normalized = f_normalize_cycle(close, price_min, price_max)
price_slope = f_slope(close, slope_lookback)
price_slope_normalized = f_slope(price_normalized, slope_lookback)

// =============================================================================
// ALGORITHM 1: EHLERS BANDPASS FILTER
// =============================================================================

ehlers_bandpass(float src, int period, float bandwidth) =>
    float delta = bandwidth
    float beta_val = math.cos(2 * math.pi / period)
    float gamma_val = 1 / math.cos(2 * math.pi * delta / period)
    float alpha_val = gamma_val - math.sqrt(math.max(gamma_val * gamma_val - 1, 0))
    var float bp = 0.0
    var float bp1 = 0.0
    var float bp2 = 0.0
    float src_safe = f_nz(src, f_nz(src[1], close))
    float src2_safe = f_nz(src[2], src_safe)
    bp := 0.5 * (1 - alpha_val) * (src_safe - src2_safe) + beta_val * (1 + alpha_val) * bp1 - alpha_val * bp2
    bp := f_nz(bp, 0.0)
    bp2 := bp1
    bp1 := bp
    bp

ehlers_bp_10 = ehlers_bandpass(close, 10, 0.3)
ehlers_bp_15 = ehlers_bandpass(close, 15, 0.3)
ehlers_bp_20 = ehlers_bandpass(close, 20, 0.3)

ehlers_combined = (f_nz(ehlers_bp_10, 0.0) + f_nz(ehlers_bp_15, 0.0) + f_nz(ehlers_bp_20, 0.0)) / 3
ehlers_min = ta.lowest(ehlers_combined, fit_window)
ehlers_max = ta.highest(ehlers_combined, fit_window)
ehlers_cycle = f_normalize_cycle(ehlers_combined, ehlers_min, ehlers_max)
ehlers_slope = f_slope(ehlers_cycle, slope_lookback)
ehlers_prob = (ehlers_cycle + 1) / 2

// =============================================================================
// ALGORITHM 2: ZERO-LAG EMA
// =============================================================================

zlema(float src, int length) =>
    float ema1 = ta.ema(src, length)
    float ema2 = ta.ema(ema1, length)
    2 * f_nz(ema1, src) - f_nz(ema2, src)

zlema_13 = zlema(close, 13)
zlema_diff = close - f_nz(zlema_13, close)
zlema_min = ta.lowest(zlema_diff, fit_window)
zlema_max = ta.highest(zlema_diff, fit_window)
zlema_cycle = f_normalize_cycle(zlema_diff, zlema_min, zlema_max)
zlema_slope = f_slope(zlema_cycle, slope_lookback)
zlema_prob = (zlema_cycle + 1) / 2

// =============================================================================
// ALGORITHM 3: COPPOCK CURVE
// =============================================================================

roc_custom(float src, int length) =>
    float src_lag = f_nz(src[length], src)
    length > 0 and src_lag != 0 ? (src - src_lag) / src_lag * 100 : 0.0

coppock_roc_short = roc_custom(close, 11)
coppock_roc_long = roc_custom(close, 14)
coppock_combined_raw = f_nz(coppock_roc_short, 0.0) + f_nz(coppock_roc_long, 0.0)
coppock = ta.wma(coppock_combined_raw, 10)
coppock_min = ta.lowest(coppock, fit_window)
coppock_max = ta.highest(coppock, fit_window)
coppock_cycle = f_normalize_cycle(coppock, coppock_min, coppock_max)
coppock_slope = f_slope(coppock_cycle, slope_lookback)
coppock_prob = (coppock_cycle + 1) / 2

// =============================================================================
// ALGORITHM 4: DETRENDED PRICE OSCILLATOR
// =============================================================================

dpo_period_val = 15
dpo_shift = math.floor(dpo_period_val / 2) + 1
dpo_sma = ta.sma(close, dpo_period_val)
dpo = close - f_nz(dpo_sma[dpo_shift], close)
dpo_min = ta.lowest(dpo, fit_window)
dpo_max = ta.highest(dpo, fit_window)
dpo_cycle = f_normalize_cycle(dpo, dpo_min, dpo_max)
dpo_slope = f_slope(dpo_cycle, slope_lookback)
dpo_prob = (dpo_cycle + 1) / 2

// =============================================================================
// ALGORITHM 5: SCHAFF TREND CYCLE
// =============================================================================

schaff_tc(float src, int fast, int slow, int cycle_len) =>
    float ema_fast = ta.ema(src, fast)
    float ema_slow = ta.ema(src, slow)
    float macd_line = f_nz(ema_fast, src) - f_nz(ema_slow, src)
    float macd_low = ta.lowest(macd_line, cycle_len)
    float macd_high = ta.highest(macd_line, cycle_len)
    float macd_range = macd_high - macd_low
    float stoch1 = macd_range > 0 ? 100 * (macd_line - macd_low) / macd_range : 50
    float stoch1_smooth = ta.ema(stoch1, 3)
    float stc_low = ta.lowest(stoch1_smooth, cycle_len)
    float stc_high = ta.highest(stoch1_smooth, cycle_len)
    float stc_range = stc_high - stc_low
    float stc = stc_range > 0 ? 100 * (stoch1_smooth - stc_low) / stc_range : 50
    ta.ema(stc, 3)

schaff = schaff_tc(close, 23, 50, 10)
schaff_cycle = (f_nz(schaff, 50.0) - 50) / 50  // Normalize to [-1, 1]
schaff_slope = f_slope(schaff_cycle, slope_lookback)
schaff_prob = (schaff_cycle + 1) / 2

// =============================================================================
// ALGORITHM 6: FISHER TRANSFORM
// =============================================================================

fisher_transform(float src, int period) =>
    float h = ta.highest(src, period)
    float l = ta.lowest(src, period)
    float pr = h - l
    float norm = pr > 0 ? 2 * ((src - l) / pr) - 1 : 0
    norm := f_clip(norm, -0.999, 0.999)
    float fisher_val = 0.5 * math.log((1 + norm) / (1 - norm))
    ta.ema(f_nz(fisher_val, 0.0), 3)

fisher = fisher_transform(close, 10)
fisher_min = ta.lowest(fisher, fit_window)
fisher_max = ta.highest(fisher, fit_window)
fisher_cycle = f_normalize_cycle(fisher, fisher_min, fisher_max)
fisher_slope = f_slope(fisher_cycle, slope_lookback)
fisher_prob = (fisher_cycle + 1) / 2

// =============================================================================
// ALGORITHM 7: MESA ADAPTIVE
// =============================================================================

mesa_sma = ta.sma(close, 20)
mesa_detrended = close - f_nz(mesa_sma, close)
var int mesa_crossings = 0
mesa_crossings := 0
for i = 0 to 19
    float md_curr = f_nz(mesa_detrended[i], 0.0)
    float md_prev = f_nz(mesa_detrended[i+1], 0.0)
    if (md_curr > 0 and md_prev < 0) or (md_curr < 0 and md_prev > 0)
        mesa_crossings := mesa_crossings + 1

mesa_period = mesa_crossings > 0 ? math.max(5.0, math.min(30.0, 40.0 / mesa_crossings)) : 10.0
mesa_cycle = math.sin((bar_index % math.round(mesa_period)) / mesa_period * 2 * math.pi)
mesa_slope = f_slope(mesa_cycle, slope_lookback)
mesa_prob = (mesa_cycle + 1) / 2

// =============================================================================
// ALGORITHM 8: GOERTZEL
// =============================================================================

goertzel_magnitude(float src, int period, int window) =>
    float omega = 2 * math.pi / period
    float coeff = 2 * math.cos(omega)
    var float s0 = 0.0
    var float s1 = 0.0
    var float s2 = 0.0
    s0 := 0.0
    s1 := 0.0
    s2 := 0.0
    for i = 0 to window - 1
        s0 := f_nz(src[i], close) + coeff * s1 - s2
        s2 := s1
        s1 := s0
    float real_part = s1 - s2 * math.cos(omega)
    float imag_part = s2 * math.sin(omega)
    math.sqrt(real_part * real_part + imag_part * imag_part)

goertzel_10 = goertzel_magnitude(close, 10, 20)
goertzel_min = ta.lowest(goertzel_10, fit_window)
goertzel_max = ta.highest(goertzel_10, fit_window)
goertzel_cycle = f_normalize_cycle(goertzel_10, goertzel_min, goertzel_max)
goertzel_slope = f_slope(goertzel_cycle, slope_lookback)
goertzel_prob = (goertzel_cycle + 1) / 2

// =============================================================================
// ALGORITHM 9: HILBERT AMPLITUDE
// =============================================================================

hilbert_upper = ta.highest(close, 15)
hilbert_lower = ta.lowest(close, 15)
hilbert_mid = (f_nz(hilbert_upper, close) + f_nz(hilbert_lower, close)) / 2
hilbert_pos = close - hilbert_mid
hilbert_min = ta.lowest(hilbert_pos, fit_window)
hilbert_max = ta.highest(hilbert_pos, fit_window)
hilbert_cycle = f_normalize_cycle(hilbert_pos, hilbert_min, hilbert_max)
hilbert_slope = f_slope(hilbert_cycle, slope_lookback)
hilbert_prob = (hilbert_cycle + 1) / 2

// =============================================================================
// ALGORITHM 10: AUTOCORRELATION
// =============================================================================

returns = close - f_nz(close[1], close)
autocorr_5_raw = ta.correlation(returns, f_nz(returns[5], 0.0), 30)
autocorr_10_raw = ta.correlation(returns, f_nz(returns[10], 0.0), 30)
autocorr_5 = f_nz(autocorr_5_raw, 0.0)
autocorr_10 = f_nz(autocorr_10_raw, 0.0)
autocorr_combined = autocorr_5 + autocorr_10
autocorr_cycle = f_clip(autocorr_combined / 2, -1.0, 1.0)
autocorr_slope = f_slope(autocorr_cycle, slope_lookback)
autocorr_prob = (autocorr_cycle + 1) / 2

// =============================================================================
// ALGORITHM 11: SSA (Simplified)
// =============================================================================

ssa_trend_calc(float src, int window) =>
    float wt_sum = 0.0
    float val_sum = 0.0
    for i = 0 to window - 1
        float weight = float(window - i)
        wt_sum := wt_sum + weight
        val_sum := val_sum + f_nz(src[i], src) * weight
    wt_sum > 0 ? val_sum / wt_sum : src

ssa_trend = ssa_trend_calc(close, 15)
ssa_residual = close - f_nz(ssa_trend, close)
ssa_min = ta.lowest(ssa_residual, fit_window)
ssa_max = ta.highest(ssa_residual, fit_window)
ssa_cycle = f_normalize_cycle(ssa_residual, ssa_min, ssa_max)
ssa_slope = f_slope(ssa_cycle, slope_lookback)
ssa_prob = (ssa_cycle + 1) / 2

// =============================================================================
// ALGORITHM 12: WAVELET (Simplified)
// =============================================================================

wv_approx = ta.sma(close, 8)
wv_detail1 = close - f_nz(wv_approx, close)
wv_approx_16 = ta.sma(close, 16)
wv_detail2 = f_nz(wv_approx, close) - f_nz(wv_approx_16, close)
wv_combined = wv_detail1 + wv_detail2 * 0.5
wv_min = ta.lowest(wv_combined, fit_window)
wv_max = ta.highest(wv_combined, fit_window)
wavelet_cycle = f_normalize_cycle(wv_combined, wv_min, wv_max)
wavelet_slope = f_slope(wavelet_cycle, slope_lookback)
wavelet_prob = (wavelet_cycle + 1) / 2

// =============================================================================
// ALGORITHM 13: EMD (Simplified)
// =============================================================================

emd_imf(float src, int period) =>
    float upper_env = ta.highest(src, period)
    float lower_env = ta.lowest(src, period)
    float mean_env = (f_nz(upper_env, src) + f_nz(lower_env, src)) / 2
    src - mean_env

emd_imf0 = emd_imf(close, 5)
emd_imf1 = emd_imf(close, 10)
emd_combined = emd_imf0 + emd_imf1 * 0.7
emd_min = ta.lowest(emd_combined, fit_window)
emd_max = ta.highest(emd_combined, fit_window)
emd_cycle = f_normalize_cycle(emd_combined, emd_min, emd_max)
emd_slope = f_slope(emd_cycle, slope_lookback)
emd_prob = (emd_cycle + 1) / 2

// =============================================================================
// GBM STATE INITIALIZATION
// =============================================================================

var array<float> gbm_weights = array.new_float(NUM_MODELS, DEFAULT_WEIGHT)
var array<float> gbm_prev_weights = array.new_float(NUM_MODELS, DEFAULT_WEIGHT)
var array<float> slope_fit_scores = array.new_float(NUM_MODELS, 0.0)
var array<float> rolling_fit_scores = array.new_float(NUM_MODELS, 0.5)
var int gbm_iteration = 0
var bool is_warmed_up = false

is_warmed_up := bar_index >= warmup_period

// =============================================================================
// SLOPE FIT CALCULATION - CORE METRIC
// =============================================================================

// Calculate how well each cycle's slope matches price slope

// 1. Correlation of cycle with normalized price over fit_window
corr_ehlers = f_rolling_corr(ehlers_cycle, price_normalized, fit_window)
corr_zlema = f_rolling_corr(zlema_cycle, price_normalized, fit_window)
corr_coppock = f_rolling_corr(coppock_cycle, price_normalized, fit_window)
corr_dpo = f_rolling_corr(dpo_cycle, price_normalized, fit_window)
corr_schaff = f_rolling_corr(schaff_cycle, price_normalized, fit_window)
corr_fisher = f_rolling_corr(fisher_cycle, price_normalized, fit_window)
corr_mesa = f_rolling_corr(mesa_cycle, price_normalized, fit_window)
corr_goertzel = f_rolling_corr(goertzel_cycle, price_normalized, fit_window)
corr_hilbert = f_rolling_corr(hilbert_cycle, price_normalized, fit_window)
corr_autocorr = f_rolling_corr(autocorr_cycle, price_normalized, fit_window)
corr_ssa = f_rolling_corr(ssa_cycle, price_normalized, fit_window)
corr_wavelet = f_rolling_corr(wavelet_cycle, price_normalized, fit_window)
corr_emd = f_rolling_corr(emd_cycle, price_normalized, fit_window)

// 2. Direction match: cycle slope same sign as price slope
f_direction_match(float cycle_slope, float price_sl) =>
    bool same_dir = (cycle_slope > 0 and price_sl > 0) or (cycle_slope < 0 and price_sl < 0)
    same_dir ? 1.0 : 0.0

dir_ehlers = f_direction_match(ehlers_slope, price_slope_normalized)
dir_zlema = f_direction_match(zlema_slope, price_slope_normalized)
dir_coppock = f_direction_match(coppock_slope, price_slope_normalized)
dir_dpo = f_direction_match(dpo_slope, price_slope_normalized)
dir_schaff = f_direction_match(schaff_slope, price_slope_normalized)
dir_fisher = f_direction_match(fisher_slope, price_slope_normalized)
dir_mesa = f_direction_match(mesa_slope, price_slope_normalized)
dir_goertzel = f_direction_match(goertzel_slope, price_slope_normalized)
dir_hilbert = f_direction_match(hilbert_slope, price_slope_normalized)
dir_autocorr = f_direction_match(autocorr_slope, price_slope_normalized)
dir_ssa = f_direction_match(ssa_slope, price_slope_normalized)
dir_wavelet = f_direction_match(wavelet_slope, price_slope_normalized)
dir_emd = f_direction_match(emd_slope, price_slope_normalized)

// 3. Combined fit score: correlation + direction bonus
f_fit_score(float corr, float dir_match) =>
    float corr_score = (corr + 1) / 2  // Convert [-1, 1] to [0, 1]
    float dir_score = dir_match * direction_bonus
    correlation_weight * corr_score + (1 - correlation_weight) * (dir_score / direction_bonus)

fit_ehlers = f_fit_score(corr_ehlers, dir_ehlers)
fit_zlema = f_fit_score(corr_zlema, dir_zlema)
fit_coppock = f_fit_score(corr_coppock, dir_coppock)
fit_dpo = f_fit_score(corr_dpo, dir_dpo)
fit_schaff = f_fit_score(corr_schaff, dir_schaff)
fit_fisher = f_fit_score(corr_fisher, dir_fisher)
fit_mesa = f_fit_score(corr_mesa, dir_mesa)
fit_goertzel = f_fit_score(corr_goertzel, dir_goertzel)
fit_hilbert = f_fit_score(corr_hilbert, dir_hilbert)
fit_autocorr = f_fit_score(corr_autocorr, dir_autocorr)
fit_ssa = f_fit_score(corr_ssa, dir_ssa)
fit_wavelet = f_fit_score(corr_wavelet, dir_wavelet)
fit_emd = f_fit_score(corr_emd, dir_emd)

// =============================================================================
// GBM AGGRESSIVE WEIGHT OPTIMIZATION
// =============================================================================

if is_warmed_up
    // Store previous weights
    for i = 0 to NUM_MODELS - 1
        array.set(gbm_prev_weights, i, f_nz(array.get(gbm_weights, i), DEFAULT_WEIGHT))
    
    // Update fit scores with exponential moving average for stability
    float decay = gbm_memory_decay
    array.set(slope_fit_scores, 0, fit_ehlers)
    array.set(slope_fit_scores, 1, fit_zlema)
    array.set(slope_fit_scores, 2, fit_coppock)
    array.set(slope_fit_scores, 3, fit_dpo)
    array.set(slope_fit_scores, 4, fit_schaff)
    array.set(slope_fit_scores, 5, fit_fisher)
    array.set(slope_fit_scores, 6, fit_mesa)
    array.set(slope_fit_scores, 7, fit_goertzel)
    array.set(slope_fit_scores, 8, fit_hilbert)
    array.set(slope_fit_scores, 9, fit_autocorr)
    array.set(slope_fit_scores, 10, fit_ssa)
    array.set(slope_fit_scores, 11, fit_wavelet)
    array.set(slope_fit_scores, 12, fit_emd)
    
    // Update rolling scores with aggressive learning
    for i = 0 to NUM_MODELS - 1
        float old_score = array.get(rolling_fit_scores, i)
        float new_score = array.get(slope_fit_scores, i)
        float updated = (1 - gbm_learning_rate) * old_score + gbm_learning_rate * new_score
        array.set(rolling_fit_scores, i, updated)
    
    // Calculate performance-based raw weights
    var array<float> perf_scores = array.new_float(NUM_MODELS, 0.0)
    float min_score = 1.0
    float max_score = 0.0
    
    for i = 0 to NUM_MODELS - 1
        float score = array.get(rolling_fit_scores, i)
        if score < min_score
            min_score := score
        if score > max_score
            max_score := score
    
    float score_range = math.max(max_score - min_score, 0.01)
    
    // Aggressive exponential scaling of scores
    for i = 0 to NUM_MODELS - 1
        float score = array.get(rolling_fit_scores, i)
        float norm_score = (score - min_score) / score_range
        // Cubic scaling for aggressive differentiation
        float exp_score = math.pow(norm_score + 0.05, 4.0)
        
        // Big bonus for high fit scores (>0.65)
        if score > 0.65
            exp_score := exp_score * (1.0 + (score - 0.65) * 8.0)
        // Heavy penalty for low fit scores (<0.4)
        if score < 0.4
            exp_score := exp_score * 0.05
        
        array.set(perf_scores, i, exp_score)
    
    // Sort to find top 3
    var array<int> sorted_indices = array.new_int(NUM_MODELS, 0)
    var array<float> temp_scores = array.new_float(NUM_MODELS, 0.0)
    
    for i = 0 to NUM_MODELS - 1
        array.set(sorted_indices, i, i)
        array.set(temp_scores, i, array.get(perf_scores, i))
    
    for pass = 0 to NUM_MODELS - 2
        for i = 0 to NUM_MODELS - 2 - pass
            if array.get(temp_scores, i) < array.get(temp_scores, i + 1)
                float tmp_score = array.get(temp_scores, i)
                array.set(temp_scores, i, array.get(temp_scores, i + 1))
                array.set(temp_scores, i + 1, tmp_score)
                int tmp_idx = array.get(sorted_indices, i)
                array.set(sorted_indices, i, array.get(sorted_indices, i + 1))
                array.set(sorted_indices, i + 1, tmp_idx)
    
    // Assign weights: Top 3 get max_top3_weight, rest share remainder
    var array<float> new_weights = array.new_float(NUM_MODELS, 0.0)
    
    float top3_score_total = 0.0
    for rank = 0 to 2
        int idx = array.get(sorted_indices, rank)
        top3_score_total += array.get(perf_scores, idx)
    top3_score_total := math.max(top3_score_total, 0.001)
    
    float bottom_score_total = 0.0
    for rank = 3 to NUM_MODELS - 1
        int idx = array.get(sorted_indices, rank)
        bottom_score_total += array.get(perf_scores, idx)
    bottom_score_total := math.max(bottom_score_total, 0.001)
    
    // Top 3 share max_top3_weight proportionally
    for rank = 0 to 2
        int idx = array.get(sorted_indices, rank)
        float score = array.get(perf_scores, idx)
        float weight = max_top3_weight * (score / top3_score_total)
        weight := math.min(weight, max_single_weight)
        array.set(new_weights, idx, weight)
    
    float actual_top3 = 0.0
    for rank = 0 to 2
        int idx = array.get(sorted_indices, rank)
        actual_top3 += array.get(new_weights, idx)
    
    // Bottom algorithms share remaining weight
    float remaining_weight = 1.0 - actual_top3
    for rank = 3 to NUM_MODELS - 1
        int idx = array.get(sorted_indices, rank)
        float score = array.get(perf_scores, idx)
        float weight = remaining_weight * (score / bottom_score_total)
        weight := math.max(weight, min_weight_floor)
        array.set(new_weights, idx, weight)
    
    // Normalize
    float total_weight = 0.0
    for i = 0 to NUM_MODELS - 1
        total_weight += array.get(new_weights, i)
    total_weight := math.max(total_weight, 0.001)
    
    for i = 0 to NUM_MODELS - 1
        float w = array.get(new_weights, i) / total_weight
        array.set(new_weights, i, w)
    
    // Apply minimal smoothing for fast adaptation
    for i = 0 to NUM_MODELS - 1
        float new_w = array.get(new_weights, i)
        float old_w = array.get(gbm_prev_weights, i)
        float smoothed_w = (1.0 - gbm_smoothing) * new_w + gbm_smoothing * old_w
        array.set(gbm_weights, i, smoothed_w)
    
    // Final normalization
    float final_total = 0.0
    for i = 0 to NUM_MODELS - 1
        final_total += array.get(gbm_weights, i)
    final_total := math.max(final_total, 0.001)
    
    for i = 0 to NUM_MODELS - 1
        float w = array.get(gbm_weights, i) / final_total
        array.set(gbm_weights, i, w)
    
    gbm_iteration := gbm_iteration + 1

// =============================================================================
// EXTRACT FINAL WEIGHTS
// =============================================================================

w_ehlers = f_nz(array.get(gbm_weights, 0), DEFAULT_WEIGHT)
w_zlema = f_nz(array.get(gbm_weights, 1), DEFAULT_WEIGHT)
w_coppock = f_nz(array.get(gbm_weights, 2), DEFAULT_WEIGHT)
w_dpo = f_nz(array.get(gbm_weights, 3), DEFAULT_WEIGHT)
w_schaff = f_nz(array.get(gbm_weights, 4), DEFAULT_WEIGHT)
w_fisher = f_nz(array.get(gbm_weights, 5), DEFAULT_WEIGHT)
w_mesa = f_nz(array.get(gbm_weights, 6), DEFAULT_WEIGHT)
w_goertzel = f_nz(array.get(gbm_weights, 7), DEFAULT_WEIGHT)
w_hilbert = f_nz(array.get(gbm_weights, 8), DEFAULT_WEIGHT)
w_autocorr = f_nz(array.get(gbm_weights, 9), DEFAULT_WEIGHT)
w_ssa = f_nz(array.get(gbm_weights, 10), DEFAULT_WEIGHT)
w_wavelet = f_nz(array.get(gbm_weights, 11), DEFAULT_WEIGHT)
w_emd = f_nz(array.get(gbm_weights, 12), DEFAULT_WEIGHT)

weight_sum = w_ehlers + w_zlema + w_coppock + w_dpo + w_schaff + w_fisher + w_mesa + w_goertzel + w_hilbert + w_autocorr + w_ssa + w_wavelet + w_emd
weight_sum := math.max(weight_sum, 0.001)

w_ehlers := w_ehlers / weight_sum
w_zlema := w_zlema / weight_sum
w_coppock := w_coppock / weight_sum
w_dpo := w_dpo / weight_sum
w_schaff := w_schaff / weight_sum
w_fisher := w_fisher / weight_sum
w_mesa := w_mesa / weight_sum
w_goertzel := w_goertzel / weight_sum
w_hilbert := w_hilbert / weight_sum
w_autocorr := w_autocorr / weight_sum
w_ssa := w_ssa / weight_sum
w_wavelet := w_wavelet / weight_sum
w_emd := w_emd / weight_sum

// =============================================================================
// WEIGHTED COMBINED CYCLE
// =============================================================================

combined_cycle = (ehlers_cycle * w_ehlers + 
                  zlema_cycle * w_zlema + 
                  coppock_cycle * w_coppock + 
                  dpo_cycle * w_dpo + 
                  schaff_cycle * w_schaff + 
                  fisher_cycle * w_fisher + 
                  mesa_cycle * w_mesa + 
                  goertzel_cycle * w_goertzel + 
                  hilbert_cycle * w_hilbert + 
                  autocorr_cycle * w_autocorr + 
                  ssa_cycle * w_ssa + 
                  wavelet_cycle * w_wavelet + 
                  emd_cycle * w_emd)

combined_cycle := f_clip(combined_cycle, -1.0, 1.0)
smoothed_cycle = ta.ema(combined_cycle, 3)
smoothed_cycle := f_nz(smoothed_cycle, 0.0)

// Combined cycle slope
combined_slope = f_slope(smoothed_cycle, slope_lookback)

// Correlation of combined cycle with price
combined_corr = f_rolling_corr(smoothed_cycle, price_normalized, fit_window)
combined_dir_match = f_direction_match(combined_slope, price_slope_normalized)
combined_fit = f_fit_score(combined_corr, combined_dir_match)

// =============================================================================
// ENSEMBLE PROBABILITY (for compatibility)
// =============================================================================

ensemble_prob = (smoothed_cycle + 1) / 2
ensemble_prob := f_clip(ensemble_prob, 0.0, 1.0)

// =============================================================================
// STATISTICS
// =============================================================================

count_up = (ehlers_prob > 0.5 ? 1 : 0) + (zlema_prob > 0.5 ? 1 : 0) + 
           (coppock_prob > 0.5 ? 1 : 0) + (dpo_prob > 0.5 ? 1 : 0) + 
           (schaff_prob > 0.5 ? 1 : 0) + (fisher_prob > 0.5 ? 1 : 0) + 
           (mesa_prob > 0.5 ? 1 : 0) + (goertzel_prob > 0.5 ? 1 : 0) + 
           (hilbert_prob > 0.5 ? 1 : 0) + (autocorr_prob > 0.5 ? 1 : 0) + 
           (ssa_prob > 0.5 ? 1 : 0) + (wavelet_prob > 0.5 ? 1 : 0) + 
           (emd_prob > 0.5 ? 1 : 0)

pct_bullish = count_up / 13.0
agreement = math.max(pct_bullish, 1 - pct_bullish)
confidence = math.abs(smoothed_cycle)

regime_type = smoothed_cycle < -0.5 ? "STRONG DOWN" :
              smoothed_cycle < -0.2 ? "BEARISH" :
              smoothed_cycle > 0.5 ? "STRONG UP" :
              smoothed_cycle > 0.2 ? "BULLISH" : "NEUTRAL"

// Find top 3 for display
var array<int> top_indices = array.new_int(3, 0)
var array<float> display_weights = array.new_float(NUM_MODELS, 0.0)

array.set(display_weights, 0, w_ehlers)
array.set(display_weights, 1, w_zlema)
array.set(display_weights, 2, w_coppock)
array.set(display_weights, 3, w_dpo)
array.set(display_weights, 4, w_schaff)
array.set(display_weights, 5, w_fisher)
array.set(display_weights, 6, w_mesa)
array.set(display_weights, 7, w_goertzel)
array.set(display_weights, 8, w_hilbert)
array.set(display_weights, 9, w_autocorr)
array.set(display_weights, 10, w_ssa)
array.set(display_weights, 11, w_wavelet)
array.set(display_weights, 12, w_emd)

var array<float> temp_display = array.new_float(NUM_MODELS, 0.0)
for i = 0 to NUM_MODELS - 1
    array.set(temp_display, i, array.get(display_weights, i))

for rank = 0 to 2
    float max_w = -1.0
    int max_idx = 0
    for i = 0 to NUM_MODELS - 1
        float w = array.get(temp_display, i)
        if w > max_w
            max_w := w
            max_idx := i
    array.set(top_indices, rank, max_idx)
    array.set(temp_display, max_idx, -999.0)

weight_entropy = f_calcEntropy(display_weights)
top1_weight = array.max(display_weights)
top3_weight = 0.0
for rank = 0 to 2
    int idx = array.get(top_indices, rank)
    top3_weight += array.get(display_weights, idx)

// =============================================================================
// PLOTTING: CYCLE AND PRICE VISUALIZATION
// =============================================================================

hline(0, "Zero", color=color.gray, linestyle=hline.style_dashed)
hline(0.5, "+0.5", color=color.new(color.green, 70), linestyle=hline.style_dotted)
hline(-0.5, "-0.5", color=color.new(color.red, 70), linestyle=hline.style_dotted)
hline(1, "Max", color=color.new(color.gray, 85))
hline(-1, "Min", color=color.new(color.gray, 85))

// Background for regime
bgcolor_color = regime_type == "STRONG DOWN" ? color.new(color.red, 90) : regime_type == "BEARISH" ? color.new(color.orange, 92) : regime_type == "STRONG UP" ? color.new(color.green, 90) : regime_type == "BULLISH" ? color.new(color.lime, 92) :  color.new(color.gray, 95)
bgcolor(bgcolor_color)

// Normalized price overlay (for visual comparison)
plot(show_price_overlay ? price_normalized : na, "Price (Normalized)", 
     color=color.new(color.blue, 40), linewidth=2, style=plot.style_line)

// Main cycle line with gradient color
cycle_color = smoothed_cycle > 0.3 ? color.green : 
              smoothed_cycle > 0 ? color.new(color.green, 50) :
              smoothed_cycle > -0.3 ? color.new(color.red, 50) : color.red
              
plot(show_cycle_line ? smoothed_cycle : na, "Combined Cycle", 
     color=cycle_color, linewidth=3)

// Fill between cycle and zero
cycle_fill_color = smoothed_cycle > 0 ? color.new(color.green, 85) : color.new(color.red, 85)
p_cycle = plot(show_cycle_line ? smoothed_cycle : na, display=display.none)
p_zero = plot(show_cycle_line ? 0.0 : na, display=display.none)
fill(p_cycle, p_zero, color=cycle_fill_color)

// Individual cycles (optional)
plot(show_individual_cycles ? ehlers_cycle * 0.8 : na, "Ehlers", 
     color=color.new(color.blue, 60), linewidth=1)
plot(show_individual_cycles ? schaff_cycle * 0.8 : na, "Schaff", 
     color=color.new(color.purple, 60), linewidth=1)
plot(show_individual_cycles ? fisher_cycle * 0.8 : na, "Fisher", 
     color=color.new(color.orange, 60), linewidth=1)

// Signal markers
bullish_cross = ta.crossover(smoothed_cycle, 0.0)
bearish_cross = ta.crossunder(smoothed_cycle, 0.0)

plotshape(bullish_cross and is_warmed_up, "Bullish Cross", 
     location=location.bottom, style=shape.triangleup, 
     size=size.small, color=color.green)
plotshape(bearish_cross and is_warmed_up, "Bearish Cross", 
     location=location.top, style=shape.triangledown, 
     size=size.small, color=color.red)

strong_bullish = smoothed_cycle > 0.5 and smoothed_cycle[1] <= 0.5
strong_bearish = smoothed_cycle < -0.5 and smoothed_cycle[1] >= -0.5
plotshape(strong_bullish and is_warmed_up, "Strong Bullish", 
     location=location.bottom, style=shape.triangleup, 
     size=size.normal, color=color.lime, text="STRONG")
plotshape(strong_bearish and is_warmed_up, "Strong Bearish", 
     location=location.top, style=shape.triangledown, 
     size=size.normal, color=color.red, text="STRONG")

// =============================================================================
// GBM STATISTICS TABLE (Bottom Left)
// =============================================================================

if show_gbm_stats and barstate.islast
    var table gbm_tbl = table.new(position.bottom_left, 2, 10, 
         bgcolor=color.new(color.white, 0), border_width=1, border_color=color.gray)
    
    table.cell(gbm_tbl, 0, 0, "GBM Slope-Fit", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(gbm_tbl, 1, 0, "Value", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    
    table.cell(gbm_tbl, 0, 1, "Iterations", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    table.cell(gbm_tbl, 1, 1, str.tostring(gbm_iteration), text_color=color.new(#B8860B, 0), text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 2, "Combined Fit", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    fit_color = combined_fit > 0.7 ? color.new(#006400, 0) : combined_fit > 0.5 ? color.new(#B8860B, 0) : color.new(#CC0000, 0)
    table.cell(gbm_tbl, 1, 2, str.tostring(math.round(combined_fit * 100, 1)) + "%", 
         text_color=fit_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 3, "Price Corr", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    corr_color = combined_corr > 0.5 ? color.new(#006400, 0) : combined_corr > 0 ? color.new(#B8860B, 0) : color.new(#CC0000, 0)
    table.cell(gbm_tbl, 1, 3, str.tostring(math.round(combined_corr, 2)), 
         text_color=corr_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 4, "Dir Match", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    dir_color = combined_dir_match > 0.5 ? color.new(#006400, 0) : color.new(#CC0000, 0)
    table.cell(gbm_tbl, 1, 4, combined_dir_match > 0.5 ? "YES" : "NO", 
         text_color=dir_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 5, "Top 1 Weight", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    t1_color = top1_weight > 0.40 ? color.new(#FF6600, 0) : top1_weight > 0.25 ? color.new(#B8860B, 0) : color.new(#006400, 0)
    table.cell(gbm_tbl, 1, 5, str.tostring(math.round(top1_weight * 100, 1)) + "%", 
         text_color=t1_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 6, "Top 3 Weight", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    t3_color = top3_weight > 0.80 ? color.new(#FF6600, 0) : top3_weight > 0.60 ? color.new(#B8860B, 0) : color.new(#006400, 0)
    table.cell(gbm_tbl, 1, 6, str.tostring(math.round(top3_weight * 100, 1)) + "%", 
         text_color=t3_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 7, "Entropy", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    ent_color = weight_entropy > 0.7 ? color.new(#006400, 0) : weight_entropy > 0.4 ? color.new(#B8860B, 0) : color.new(#FF6600, 0)
    table.cell(gbm_tbl, 1, 7, str.tostring(math.round(weight_entropy, 2)), 
         text_color=ent_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 8, "Cycle Slope", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    slope_color = combined_slope > 0 ? color.new(#006400, 0) : color.new(#CC0000, 0)
    table.cell(gbm_tbl, 1, 8, str.tostring(math.round(combined_slope, 4)), 
         text_color=slope_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    table.cell(gbm_tbl, 0, 9, "Price Slope", text_color=color.black, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    pslope_color = price_slope_normalized > 0 ? color.new(#006400, 0) : color.new(#CC0000, 0)
    table.cell(gbm_tbl, 1, 9, str.tostring(math.round(price_slope_normalized, 4)), 
         text_color=pslope_color, text_size=size.tiny, bgcolor=color.new(color.white, 0))

// =============================================================================
// WEIGHT & FIT TABLE (Bottom Right)
// =============================================================================

if show_weight_table and barstate.islast
    var table weight_tbl = table.new(position.bottom_right, 6, 18, 
         bgcolor=color.new(color.white, 0), border_width=1, border_color=color.gray)
    
    // Header
    table.cell(weight_tbl, 0, 0, "Algorithm", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 1, 0, "Fit Score", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 2, 0, "Corr", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 3, 0, "Dir", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 4, 0, "Weight", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 5, 0, "Cycle", bgcolor=color.new(color.blue, 30), 
         text_color=color.white, text_size=size.tiny)
    
    algo_names = array.from("Ehlers BP", "Zero-Lag EMA", "Coppock", "DPO", "Schaff TC", "Fisher", "MESA", "Goertzel", "Hilbert", "Autocorr", "SSA", "Wavelet", "EMD")
    
    algo_fit = array.from(fit_ehlers, fit_zlema, fit_coppock, fit_dpo, fit_schaff, fit_fisher,
                          fit_mesa, fit_goertzel, fit_hilbert, fit_autocorr, fit_ssa, fit_wavelet, fit_emd)
    
    algo_corr = array.from(corr_ehlers, corr_zlema, corr_coppock, corr_dpo, corr_schaff, corr_fisher,
                           corr_mesa, corr_goertzel, corr_hilbert, corr_autocorr, corr_ssa, corr_wavelet, corr_emd)
    
    algo_dir = array.from(dir_ehlers, dir_zlema, dir_coppock, dir_dpo, dir_schaff, dir_fisher,
                          dir_mesa, dir_goertzel, dir_hilbert, dir_autocorr, dir_ssa, dir_wavelet, dir_emd)
    
    algo_wt = array.from(w_ehlers, w_zlema, w_coppock, w_dpo, w_schaff, w_fisher,
                         w_mesa, w_goertzel, w_hilbert, w_autocorr, w_ssa, w_wavelet, w_emd)
    
    algo_cycle = array.from(ehlers_cycle, zlema_cycle, coppock_cycle, dpo_cycle, schaff_cycle, fisher_cycle, mesa_cycle, goertzel_cycle, hilbert_cycle, autocorr_cycle, ssa_cycle, wavelet_cycle, emd_cycle)
    
    for i = 0 to 12
        float fit = f_nz(array.get(algo_fit, i), 0.5)
        float corr = f_nz(array.get(algo_corr, i), 0.0)
        float dir = f_nz(array.get(algo_dir, i), 0.0)
        float wt = f_nz(array.get(algo_wt, i), DEFAULT_WEIGHT)
        float cyc = f_nz(array.get(algo_cycle, i), 0.0)
        
        fit_col = fit > 0.65 ? color.new(#006400, 0) : fit > 0.50 ? color.new(#B8860B, 0) : color.new(#CC0000, 0)
        corr_col = corr > 0.5 ? color.new(#006400, 0) : corr > 0 ? color.new(#B8860B, 0) : color.new(#CC0000, 0)
        dir_col = dir > 0.5 ? color.new(#006400, 0) : color.new(#CC0000, 0)
        wt_col = wt > 0.15 ? color.new(#00AA00, 0) : wt > 0.08 ? color.new(#006400, 0) : wt > 0.03 ? color.new(#B8860B, 0) : color.new(#666666, 0)
        cyc_col = cyc > 0 ? color.new(#006400, 0) : color.new(#CC0000, 0)
        
        row_bg = color.new(color.white, 0)
        for rank = 0 to 2
            if i == array.get(top_indices, rank)
                row_bg := color.new(color.blue, 85)
        
        table.cell(weight_tbl, 0, i + 1, array.get(algo_names, i), 
             text_color=color.black, text_size=size.tiny, bgcolor=row_bg)
        table.cell(weight_tbl, 1, i + 1, str.tostring(math.round(fit * 100, 0)) + "%", 
             text_color=fit_col, text_size=size.tiny, bgcolor=row_bg)
        table.cell(weight_tbl, 2, i + 1, str.tostring(math.round(corr, 2)), 
             text_color=corr_col, text_size=size.tiny, bgcolor=row_bg)
        table.cell(weight_tbl, 3, i + 1, dir > 0.5 ? "✓" : "✗", 
             text_color=dir_col, text_size=size.tiny, bgcolor=row_bg)
        table.cell(weight_tbl, 4, i + 1, str.tostring(math.round(wt * 100, 1)) + "%", 
             text_color=wt_col, text_size=size.tiny, bgcolor=row_bg)
        table.cell(weight_tbl, 5, i + 1, str.tostring(math.round(cyc, 2)), 
             text_color=cyc_col, text_size=size.tiny, bgcolor=row_bg)
    
    // Separator
    for col = 0 to 5
        table.cell(weight_tbl, col, 14, "─────", text_color=color.gray, text_size=size.tiny, bgcolor=color.new(color.white, 0))
    
    // Ensemble row
    table.cell(weight_tbl, 0, 15, "COMBINED", bgcolor=color.new(color.purple, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 1, 15, str.tostring(math.round(combined_fit * 100, 0)) + "%", 
         bgcolor=color.new(color.purple, 30), 
         text_color=combined_fit > 0.6 ? color.new(#00FF00, 0) : color.new(#FF4444, 0), text_size=size.tiny)
    table.cell(weight_tbl, 2, 15, str.tostring(math.round(combined_corr, 2)), 
         bgcolor=color.new(color.purple, 30), 
         text_color=combined_corr > 0.5 ? color.new(#00FF00, 0) : color.new(#FF4444, 0), text_size=size.tiny)
    table.cell(weight_tbl, 3, 15, combined_dir_match > 0.5 ? "✓" : "✗", 
         bgcolor=color.new(color.purple, 30),
         text_color=combined_dir_match > 0.5 ? color.new(#00FF00, 0) : color.new(#FF4444, 0), text_size=size.tiny)
    table.cell(weight_tbl, 4, 15, "100%", bgcolor=color.new(color.purple, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 5, 15, str.tostring(math.round(smoothed_cycle, 2)), 
         bgcolor=color.new(color.purple, 30),
         text_color=smoothed_cycle > 0 ? color.new(#00FF00, 0) : color.new(#FF4444, 0), text_size=size.tiny)
    
    // Regime row
    reg_color = regime_type == "STRONG DOWN" or regime_type == "BEARISH" ? color.new(#CC0000, 0) : regime_type == "STRONG UP" or regime_type == "BULLISH" ? color.new(#006400, 0) :  color.new(#666666, 0)
    table.cell(weight_tbl, 0, 16, "REGIME", bgcolor=color.new(color.teal, 30), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 1, 16, regime_type, bgcolor=color.new(color.teal, 30), 
         text_color=reg_color, text_size=size.tiny)
    table.cell(weight_tbl, 2, 16, str.tostring(count_up) + "/13", 
         bgcolor=color.new(color.teal, 30), text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 3, 16, "", bgcolor=color.new(color.teal, 30))
    table.cell(weight_tbl, 4, 16, str.tostring(math.round(agreement * 100)) + "% agr", 
         bgcolor=color.new(color.teal, 30), text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 5, 16, "", bgcolor=color.new(color.teal, 30))
    
    // Status row
    warmup_color = is_warmed_up ? color.new(#006400, 0) : color.new(#FF6600, 0)
    table.cell(weight_tbl, 0, 17, "STATUS", bgcolor=color.new(color.gray, 50), 
         text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 1, 17, is_warmed_up ? "FITTING" : "WARMUP", 
         bgcolor=color.new(color.gray, 50), text_color=warmup_color, text_size=size.tiny)
    table.cell(weight_tbl, 2, 17, str.tostring(fit_window) + "d window", 
         bgcolor=color.new(color.gray, 50), text_color=color.white, text_size=size.tiny)
    table.cell(weight_tbl, 3, 17, "", bgcolor=color.new(color.gray, 50))
    table.cell(weight_tbl, 4, 17, "", bgcolor=color.new(color.gray, 50))
    table.cell(weight_tbl, 5, 17, "", bgcolor=color.new(color.gray, 50))

// =============================================================================
// ALERTS
// =============================================================================

alertcondition(bullish_cross and is_warmed_up, "Cycle Bullish Cross", 
     message="MCPS: Cycle crossed above zero - BULLISH on {{ticker}}")
alertcondition(bearish_cross and is_warmed_up, "Cycle Bearish Cross", 
     message="MCPS: Cycle crossed below zero - BEARISH on {{ticker}}")
alertcondition(strong_bullish and is_warmed_up, "Strong Bullish Signal", 
     message="MCPS: STRONG BULLISH - Cycle exceeded +0.5 on {{ticker}}")
alertcondition(strong_bearish and is_warmed_up, "Strong Bearish Signal", 
     message="MCPS: STRONG BEARISH - Cycle fell below -0.5 on {{ticker}}")
alertcondition(combined_fit > 0.75 and is_warmed_up, "High Fit Quality", 
     message="MCPS: High slope fit quality (>75%) on {{ticker}}")

// =============================================================================
// DATA WINDOW PLOTS
// =============================================================================

plot(smoothed_cycle, "Combined Cycle", display=display.data_window)
plot(combined_fit * 100, "Combined Fit %", display=display.data_window)
plot(combined_corr, "Price Correlation", display=display.data_window)
plot(combined_slope, "Cycle Slope", display=display.data_window)
plot(price_slope_normalized, "Price Slope", display=display.data_window)
plot(top1_weight * 100, "Top 1 Weight %", display=display.data_window)
plot(top3_weight * 100, "Top 3 Weight %", display=display.data_window)
plot(weight_entropy, "Weight Entropy", display=display.data_window)
plot(is_warmed_up ? 1 : 0, "Active", display=display.data_window)
